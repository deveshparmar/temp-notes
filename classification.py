# -*- coding: utf-8 -*-
"""ML___LAB___ASSIGNMENT_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zEb01digzzY-fn1VnR7tQQMgHu8RxLIR
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

data = pd.read_csv("/content/drive/MyDrive/data.csv")
data.head()

# Data Preprocessing

# 1. Drop the 'id' column as it's just an identifier and 'Unnamed: 32' which contains NaN values
data = data.drop(columns=['id', 'Unnamed: 32'])

# 2. Encode the 'diagnosis' column (Malignant = 1, Benign = 0)
data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Check if there are any missing values in the dataset
missing_values = data.isnull().sum()

# Display the first few rows after preprocessing
data_head = data.head()
data_head, missing_values

from sklearn.model_selection import train_test_split

# Splitting the data into features (X) and target (y)
X = data.drop(columns=['diagnosis'])
y = data['diagnosis']

# Split the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Initialize the Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Train the model
dt_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = dt_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

accuracy, precision, recall

import matplotlib.pyplot as plt

# Data for visualization
metrics = ['Accuracy', 'Precision', 'Recall']
values = [accuracy, precision, recall]

# Plotting
plt.figure(figsize=(10, 6))
plt.bar(metrics, values, color=['blue', 'green', 'orange'])
plt.ylim(0.8, 1)
plt.title('Decision Tree Classifier Performance Metrics')
plt.ylabel('Score')
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Initialize the models
rf_classifier = RandomForestClassifier(random_state=42)
svm_classifier = SVC(random_state=42, probability=True)

# Train the models
rf_classifier.fit(X_train, y_train)
svm_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = rf_classifier.predict(X_test)
y_pred_svm = svm_classifier.predict(X_test)

# Calculate performance metrics for Random Forest
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)

# Calculate performance metrics for SVM
accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm)
recall_svm = recall_score(y_test, y_pred_svm)

(accuracy_rf, precision_rf, recall_rf), (accuracy_svm, precision_svm, recall_svm)

# Data for visualization
models = ['Decision Tree', 'Random Forest', 'SVM']
accuracies = [accuracy, accuracy_rf, accuracy_svm]
precisions = [precision, precision_rf, precision_svm]
recalls = [recall, recall_rf, recall_svm]

# Plotting
plt.figure(figsize=(14, 6))

# Accuracy comparison
plt.subplot(1, 3, 1)
plt.bar(models, accuracies, color=['blue', 'green', 'orange'])
plt.ylim(0.8, 1.1)
plt.title('Accuracy Comparison')
plt.ylabel('Score')

# Precision comparison
plt.subplot(1, 3, 2)
plt.bar(models, precisions, color=['blue', 'green', 'orange'])
plt.ylim(0.8, 1.1)
plt.title('Precision Comparison')
plt.ylabel('Score')

# Recall comparison
plt.subplot(1, 3, 3)
plt.bar(models, recalls, color=['blue', 'green', 'orange'])
plt.ylim(0.8, 1.1)
plt.title('Recall Comparison')
plt.ylabel('Score')

plt.tight_layout()
plt.show()

from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their possible values
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV with the model and hyperparameters
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the model to the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and the associated accuracy
best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_params, best_score

# Train the Random Forest Classifier with the best hyperparameters
best_rf_classifier = RandomForestClassifier(**best_params, random_state=42)
best_rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred_best_rf = best_rf_classifier.predict(X_test)

# Calculate performance metrics for the tuned Random Forest
accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)
precision_best_rf = precision_score(y_test, y_pred_best_rf)
recall_best_rf = recall_score(y_test, y_pred_best_rf)

accuracy_best_rf, precision_best_rf, recall_best_rf

from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Initialize the Min-Max scaler
min_max_scaler = MinMaxScaler()

# Scale the training and testing data
X_train_min_max = min_max_scaler.fit_transform(X_train)
X_test_min_max = min_max_scaler.transform(X_test)

# Train the Random Forest Classifier on the Min-Max scaled data
rf_classifier_min_max = RandomForestClassifier(**best_params, random_state=42)
rf_classifier_min_max.fit(X_train_min_max, y_train)

# Predict on the test set
y_pred_min_max = rf_classifier_min_max.predict(X_test_min_max)

# Calculate performance metrics for the Random Forest with Min-Max scaled data
accuracy_min_max = accuracy_score(y_test, y_pred_min_max)
precision_min_max = precision_score(y_test, y_pred_min_max)
recall_min_max = recall_score(y_test, y_pred_min_max)

accuracy_min_max, precision_min_max, recall_min_max

# Initialize the Standard scaler
standard_scaler = StandardScaler()

# Scale the training and testing data
X_train_standard = standard_scaler.fit_transform(X_train)
X_test_standard = standard_scaler.transform(X_test)

# Train the Random Forest Classifier on the Standard scaled data
rf_classifier_standard = RandomForestClassifier(**best_params, random_state=42)
rf_classifier_standard.fit(X_train_standard, y_train)

# Predict on the test set
y_pred_standard = rf_classifier_standard.predict(X_test_standard)

# Calculate performance metrics for the Random Forest with Standard scaled data
accuracy_standard = accuracy_score(y_test, y_pred_standard)
precision_standard = precision_score(y_test, y_pred_standard)
recall_standard = recall_score(y_test, y_pred_standard)

accuracy_standard, precision_standard, recall_standard

from sklearn.model_selection import cross_val_score

# Implement 5-fold cross-validation for the Random Forest Classifier
cv_scores = cross_val_score(RandomForestClassifier(**best_params, random_state=42), X, y, cv=5, scoring='accuracy', n_jobs=-1)

# Calculate average accuracy across all folds
average_cv_accuracy = cv_scores.mean()

average_cv_accuracy

